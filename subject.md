コンテンツへとスキップする
あなた:
翻訳して
ChatGPT:
了解です。このPDF全体を日本語に翻訳します。
内容は 42 の課題「Learn2Slither（強化学習）」の仕様書ですね。以下が全文翻訳です。

Learn2Slither
強化学習

概要：
試行錯誤を通して環境内での振る舞いを学習するヘビを作成する。

バージョン：1.00

目次
I はじめに
II 導入
III 一般ルール
IV 必須パート
IV.1 パート1 環境 ボード
IV.2 パート2 状態
IV.3 パート3 行動
IV.4 パート4 報酬
IV.5 パート5 Q学習
IV.6 パート6 技術構成
V 提出と相互評価
VI ボーナスパート

第I章 はじめに
宇宙と時間の神秘的なひだの中で、星々が奇妙な旋律を奏で、惑星たちが予期せぬ出会いを演出する場所がある。その宇宙の一角には「スネーキー銀河」と呼ばれる特異な領域が存在する。

この銀河は、虚空を漂う小惑星に優雅に巻き付きながら、深遠な哲学的議論を交わす地球外のヘビたちで知られている。虹色に輝く鱗を持つ彼らは、存在論的な問いを宿し、宇宙の謎、生命、そして万物について語り合う。

彼らの発するシューという音は、ねじれた概念と思想の交響曲であり、指の間をすり抜ける小惑星のように捉えがたい。

この宇宙ヘビたちは宇宙の秘密の守護者だと言う者もいれば、並行するヘビ次元の賢者だと囁く者もいる。いずれにせよ、スネーキー銀河は、宇宙の論理すら超越する優雅さで思考を絡め取りながら、生命の大いなる問いを議論し続ける場所である。

ダグラス・アダムス
銀河ヒッチハイクガイド 生命、宇宙、そして万物についての最後のレストランより

第II章 導入
このプロジェクトは、強化学習を扱う人工知能プロジェクトである。

強化学習は、エージェントが環境と相互作用しながら最適な意思決定戦略を学ぶという、人工知能分野の最前線の手法である。従来のプログラミングとは異なり、報酬や罰というフィードバックをもとに、試行錯誤を通じて振る舞いを改善していく。

本プロジェクトでは、ボード上を移動するヘビを作成し、それを知的エージェントが制御する。ボードとエージェントには、後述する明確なルールと制約が存在するため、開発を始める前に課題文全体を注意深く読むことを強く推奨する。

ヘビを制御するエージェントは強化学習を用いて意思決定を行う。各行動の結果、ボードから正または負のフィードバックが返され、エージェントは報酬を最大化するように学習を進めていく。

この学習過程は、人間や動物が経験から学ぶ方法に似ており、強化学習が多くの複雑な問題に対して有効である理由の一つである。

第III章 一般ルール
・好きなクラスター上のコンピュータ、または仮想マシンを使用してよい

仮想マシンの場合
・使用するOSは自由
・プロジェクトに必要なソフトウェアはすべて自分でインストールすること

学内コンピュータの場合
・必要なツールがすべてインストールされている、もしくはローカルにインストール可能であること
・必要な容量を確保すること（goinfre がある場合は使用すること）
・評価前にすべての準備が完了していること

・プログラムは未定義動作を除き、予期せず終了してはならない。これが起きた場合、プロジェクトは機能しないものと見なされ、評価は0となる

・ボード、エージェント、学習済みモデルを指定された Git リポジトリに提出すること
・評価時は、新規ディレクトリに git clone された状態で実行される

・使用言語は自由だが、Python を推奨する
・Python を使用する場合は norm を遵守すること
pip install flake8
alias norminette_python=flake8

・ヘビのトレーニングを楽しもう

第IV章 必須パート
IV.1 パート1 環境 ボード
ボードは以下のルールに従う。

・サイズは 10×10
・ランダムな位置に緑のリンゴが2つ
・ランダムな位置に赤いリンゴが1つ
・ヘビは長さ3で開始し、連続した形でランダム配置

・壁に衝突 → ゲームオーバー
・自分の尻尾に衝突 → ゲームオーバー
・緑のリンゴを食べる → 長さ +1、新しい緑リンゴ出現
・赤のリンゴを食べる → 長さ -1、新しい赤リンゴ出現
・長さが0 → ゲームオーバー

学習は1回では不十分なため、数百〜数千回のトレーニングセッションを行う。
実行回数はコマンドライン引数で指定できるようにすること。

グラフィカル表示が必須であり、速度調整、人間が見やすい速度、ステップ実行モードを用意すること。

色の意味
・緑：緑リンゴ
・赤：赤リンゴ
・青：ヘビ

IV.2 パート2 状態
ヘビの視界は、頭から上下左右の4方向のみ。

表示文字の意味
・W 壁
・H ヘビの頭
・S ヘビの胴体
・G 緑リンゴ
・R 赤リンゴ
・0 空マス

エージェントに渡してよい情報は、この視界情報のみ。
それ以上の情報を与えると -42 のペナルティ。

IV.3 パート3 行動
エージェントが取れる行動は4つのみ。

UP
LEFT
DOWN
RIGHT

意思決定は状態情報のみに基づくこと。
それ以外のボード情報を使ってはならない。

ボードは別ウィンドウで表示し、状態と行動はターミナルに表示する。

IV.4 パート4 報酬
目標
・長さ10以上に到達
・できるだけ長く生存する

報酬設計は自由だが、例として以下が考えられる。

・赤リンゴを食べる → 負の報酬
・緑リンゴを食べる → 正の報酬
・何も食べない → 小さな負の報酬
・ゲームオーバー → 大きな負の報酬

報酬は将来同じ状況で同じ行動を選ぶ確率に影響する。

IV.5 パート5 Q学習
Q関数を用いて、状態に対する行動の価値を評価するモデルを実装すること。

・Qテーブル または ニューラルネットワーク可
・それ以外のモデルは禁止（0点）

Q関数の更新
・行動後に得られた報酬をもとに更新する

探索と活用
・最適と思われる行動だけでなく、ランダム行動も必要

反復学習
・行動 → 報酬 → 更新 を繰り返す
・セッション終了後も新しいセッションで学習を継続

モデルの保存と読み込み
・Q値を含む1つのファイルとして保存
・同じファイルを読み込んで学習状態を復元可能

学習停止モード
・報酬を無視し、Q関数を更新しないモード
・モデルの性能評価用

学習中は表示を無効にして高速化してよい。

IV.6 パート6 技術構成
評価しやすいよう、モジュール構成を明確に分けること。
通信方法は自由。

第V章 提出と相互評価
・ボード、エージェント、モデルを Git リポジトリに提出
・models フォルダに学習済みモデルを保存

・1回、10回、100回学習したモデルを最低3つ用意すること

・保存済みモデルを読み込み、学習なしモードで評価できること

目標
・最終的に長さ10以上
・生存時間が長いこと

第VI章 ボーナス
必須パートをすべて満たした場合のみ評価。

例
・最終長が 15, 20, 25, 30, 35
・高品質なUI ロビー 設定 統計表示
・ボードサイズ可変（学習済みモデルを再利用可能）

もし次に
・要点だけまとめてほしい
・実装方針を噛み砕いて説明してほしい
・State設計やQテーブルのサイズ感を整理したい

などあれば、そこも一緒にやろう。

選択されていません選択されていません
